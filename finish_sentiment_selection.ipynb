{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aabcd830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bcc76c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3b44fc",
   "metadata": {},
   "source": [
    "## 处理数据集，获取content并提取以#开头和结尾的超话标题，并选出讨论人数最多的挑选出当月人们讨论最激烈的实体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f1fd5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3800/668312970.py:4: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(path, on_bad_lines='skip',encoding = \"utf-8\",quoting=csv.QUOTE_NONE) #读取文件中所有数据\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#既然做就做好#【转！向他们，说声辛苦了[心]】他们是医护工作者、是环卫工人、是爱心志愿者…...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>医生护士都倒下的时候、就都要在家等死了。 直接亡国灭种？  不战而屈人之兵——新冠nb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@徐大大大为@李凌佳琦 @耳机吧羊驼@莫家HiFi花园 @肥威   今天偶尔搜一下“扬仕”二...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>第一次在微博维权 我尽可能用平静的语气述说  我是一名意大利的武汉留学生，上海科雅国际大酒店...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>【#这10个追问美国必须回答#】100天，逾百万。自1月20日确诊首例至今，美国已成为全球新...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>越来越多的来自世界各地研究都在指向一个既严重又可怕的问题，病毒在世界各地已经有一段时间了。之...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>【#塞尔维亚为中国医疗队授最高荣誉勋章#】当地时间4月30日，塞尔维亚国防部长武林为中国援塞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>【666！来看#学霸宅家作息表#[憧憬]】近日，@河南理工大学 的学子们晒出了疫情期间的宅家...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>【666！来看#学霸宅家作息表#[憧憬]】近日，@河南理工大学 的学子们晒出了疫情期间的宅家...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#酝酿新一轮金融危机的导火索# 3月9日，全球股市、原油、黄金普遍遭遇“黑色星期一”。之后美...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content\n",
       "0  #既然做就做好#【转！向他们，说声辛苦了[心]】他们是医护工作者、是环卫工人、是爱心志愿者…...\n",
       "1        医生护士都倒下的时候、就都要在家等死了。 直接亡国灭种？  不战而屈人之兵——新冠nb\n",
       "2  @徐大大大为@李凌佳琦 @耳机吧羊驼@莫家HiFi花园 @肥威   今天偶尔搜一下“扬仕”二...\n",
       "3  第一次在微博维权 我尽可能用平静的语气述说  我是一名意大利的武汉留学生，上海科雅国际大酒店...\n",
       "4  【#这10个追问美国必须回答#】100天，逾百万。自1月20日确诊首例至今，美国已成为全球新...\n",
       "5  越来越多的来自世界各地研究都在指向一个既严重又可怕的问题，病毒在世界各地已经有一段时间了。之...\n",
       "6  【#塞尔维亚为中国医疗队授最高荣誉勋章#】当地时间4月30日，塞尔维亚国防部长武林为中国援塞...\n",
       "7  【666！来看#学霸宅家作息表#[憧憬]】近日，@河南理工大学 的学子们晒出了疫情期间的宅家...\n",
       "8  【666！来看#学霸宅家作息表#[憧憬]】近日，@河南理工大学 的学子们晒出了疫情期间的宅家...\n",
       "9  #酝酿新一轮金融危机的导火索# 3月9日，全球股市、原油、黄金普遍遭遇“黑色星期一”。之后美..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "path = '2020-05.csv'\n",
    "\n",
    "# 使用pandas读入\n",
    "data = pd.read_csv(path, on_bad_lines='skip',encoding = \"utf-8\",quoting=csv.QUOTE_NONE) #读取文件中所有数据\n",
    "x = data[['content']]#读取某一列\n",
    "\n",
    "x.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于每一行，通过列名name访问对应的元素\n",
    "#for index, row in data.iterrows():\n",
    "#    print(row['content']) # 输出每一行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86acd62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_nonChinese(text):\n",
    "    pattern = re.compile(r'[\\u4e00-\\u9fa5]+')\n",
    "    result = pattern.findall(text)\n",
    "    return \" \".join(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534ee6c",
   "metadata": {},
   "source": [
    "提取以#开头和结尾的超话名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff297175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#高以翔去世#\n"
     ]
    }
   ],
   "source": [
    "chaohua_list = []\n",
    "chaohua_dic = {}\n",
    "for index, row in data.iterrows():\n",
    "    chaohua = re.findall('#.*?#',row['content']) \n",
    "    chaohua_list.extend(chaohua)\n",
    "print(chaohua_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f5269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c803e26",
   "metadata": {},
   "source": [
    "删去超话中的#，计算频数并统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deb8d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "for i in chaohua_list:\n",
    "    i = i.replace('#','')\n",
    "    if i in chaohua_dic.keys():\n",
    "        chaohua_dic[i] = chaohua_dic[i] + 1\n",
    "    else:\n",
    "        chaohua_dic[i] = 1\n",
    "#print(chaohua_dic)\n",
    "sorted_dic = dict(sorted(chaohua_dic.items(),key = operator.itemgetter(1), reverse=True))\n",
    "print(sorted_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e46412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2020-3.txt','w') as f:\n",
    "    \n",
    "    for i, (k, v) in enumerate(sorted_dic.items()):\n",
    "        if i in range(0, 300):\n",
    "            f.write(k + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dee158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Requirement already satisfied: stanfordcorenlp in d:\\python\\anaconda3\\lib\\site-packages (3.9.1.1)\n",
      "Requirement already satisfied: psutil in d:\\python\\anaconda3\\lib\\site-packages (from stanfordcorenlp) (5.9.0)\n",
      "Requirement already satisfied: requests in d:\\python\\anaconda3\\lib\\site-packages (from stanfordcorenlp) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\python\\anaconda3\\lib\\site-packages (from requests->stanfordcorenlp) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\anaconda3\\lib\\site-packages (from requests->stanfordcorenlp) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\anaconda3\\lib\\site-packages (from requests->stanfordcorenlp) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\python\\anaconda3\\lib\\site-packages (from requests->stanfordcorenlp) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install stanfordcorenlp  -i http://pypi.mirrors.ustc.edu.cn/simple/ --trusted-host pypi.mirrors.ustc.edu.cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "156ed439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba  \n",
    "import jieba.posseg\n",
    "import jieba.analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2e05e4",
   "metadata": {},
   "source": [
    "**获取所有月份的评论中的热门讨论实体**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cc790dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['武汉', '隔离', '肺炎', '口罩', '感染', '疫情', '新型冠状病毒', 'COVID19', '疫情地图', '防控', '传播', 'N95', '方舱医院', '阳性', '核酸检测', '无症状感染', '钟南山', '谣言', '症状', '野生动物', '疫苗', '中药', '双黄连口服液', '防疫', '封城', '康复', '健康码', '延期', '感染人数', '病例', '火神山', '医护人员']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# 读取实体名词文件\n",
    "with open(\"entity.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    entities = [line.strip() for line in f]\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45c89a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取停用词\n",
    "with open(\"stop_words.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    stop_words = [line.strip() for line in f]\n",
    "#print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a685f18",
   "metadata": {},
   "source": [
    "对数据进行预处理，删去超话内容以及非中文内容防止对结果造成干扰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "131a1967",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3800/2957331867.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'#.*?#'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#删去超话内容，防止干扰提取情感词\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdel_nonChinese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#删去非中文内容\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzhconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'zh-cn'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#将繁体中文变为简体中文\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m#words = [word for word in jieba.cut(text) if word not in stop_words]  #删除停用词\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtext_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/zhconv/zhconv.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(s, locale, update)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mmaxword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mmaxpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mN\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfrag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpfset\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfrag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnewset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfrag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0mmaxword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfrag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import zhconv\n",
    "\n",
    "text_list = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    #print(row['content']) \n",
    "    x = re.sub(r'#.*?#', '', row['content']) #删去超话内容，防止干扰提取情感词\n",
    "    result = del_nonChinese(x) #删去非中文内容\n",
    "    text = zhconv.convert(result, 'zh-cn')  #将繁体中文变为简体中文\n",
    "    #words = [word for word in jieba.cut(text) if word not in stop_words]  #删除停用词\n",
    "    text_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ca4d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'有朋友私下咨询 成人感冒咳嗽 咽痛咳嗽三天 医生开了这 个药 你觉得有毛病没 科普下 童爸回复 具体病情不清楚 我就说说要药物吧 咽痛咳嗽三天 这个多是感冒或者流感等病毒造成的 大概占了 以上的比例 一般来说 如果要开抗生素阿莫西林 应该提供细菌感染的证据 这里就要多问一句 医生提供了什么证据 毕竟咽痛咳嗽不是细菌感染的证据啊 其他三个药都是没有循证依据的药品 走出国门没有任何一个国家承认 我是不吃的 我最近做了一个常见的细菌感染的小合集 怀疑这些疾病且有一定证据 才应该考虑使用抗生素的 而且医生必须提供证据 细菌感染疾病笔记小汇总 细菌感染 抗生素 猩红热 链球菌性咽炎 中耳炎 鼻窦炎 肺炎 呼吸急促 尿路感染 毛囊炎 疖肿 脓疱疮 血常规 验血 反应蛋白 静脉注射 影像检查和辐射 外伤 伤口 此外你还可以儿童常见呼吸道疾病科普小合集 这个也差不多适合成人的 可以重点看看 扁桃体炎 咽炎 里面有一些缓解手段 其实一般来说没有反复高烧 呼吸急促的情况下 如果精神状态还可以 完全可以对症护理 以缓解症状为主要应对手段 例如发烧或者头疼肌肉疼可以吃非甾体抗炎药 一般首选布洛芬咀嚼片 咽痛可以看看笔记里的护理手段 一般积极护理的情况下 三四天一般症状都会明显好转然后慢慢自愈 如果过了三四天症状越来越严重 那时候还是需要再看医生的 此外 预防性使用抗生素是无效的 很多医生以预防性使用为借口 这是他们不学无术的表现 抗生素笔记里提到这点了 大数据显示了 预防性使用抗生素是无效的 甚至反倒有引起二次感染的风险 儿童常见呼吸道疾病科普 感冒 感冒药 鼻涕 咳嗽 扁桃体炎 咽炎 喉炎 支气管炎 转发理由 转发微博'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ba241f",
   "metadata": {},
   "source": [
    "## 使用jieba模型进行分词和词性标注，找出形容词和副词并将他们以副词_形容词的形式组合起来作为情感词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f64fa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.648 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "\n",
    "sentiment_dic = {}\n",
    "\n",
    "for text in text_list:\n",
    "    #print(text)\n",
    "    # 对文本进行分词和词性标注\n",
    "    words = pseg.cut(text)\n",
    "    words_with_pos = [(word, flag) for word, flag in words]\n",
    "\n",
    "    # 将实体替换为“2019-12”，如果没有实体，则删除该文本\n",
    "    has_entity = False\n",
    "    original_entity = None\n",
    "    #for text in test_list:\n",
    "    #for i in range(len(words_with_pos)):\n",
    "    #    if words_with_pos[i][0] in entities:\n",
    "    #        has_entity = True\n",
    "    #        original_entity = words_with_pos[i][0]\n",
    "    #        words_with_pos[i] = ('2020-05','entity')\n",
    "            #print(words_with_pos)\n",
    "\n",
    "    #if not has_entity:\n",
    "        #test_list.remove(text)\n",
    "\n",
    "    # 根据形容词和副词提取情感词\n",
    "    emotions = []\n",
    "    for i in range(len(words_with_pos)):\n",
    "        word, pos = words_with_pos[i]\n",
    "        if pos.startswith('a'):\n",
    "            # 如果当前词是形容词\n",
    "            if i == 0 or not words_with_pos[i-1][1].startswith('d'):\n",
    "                # 如果当前词是第一个词或前面没有副词，则将当前词加入情感词\n",
    "                emotions.append(word)\n",
    "            else:\n",
    "                # 否则，将副词和形容词组合成情感词\n",
    "                emotions.append(words_with_pos[i-1][0] + '_' + word)\n",
    "        elif pos.startswith('d'):\n",
    "            # 如果当前词是副词，不处理\n",
    "            continue\n",
    "\n",
    "    # 找到出现最频繁的情感词\n",
    "    if has_entity:\n",
    "        for emotion in emotions:\n",
    "            if emotion not in sentiment_dic.keys():\n",
    "                sentiment_dic[emotion] = 1\n",
    "            else:\n",
    "                sentiment_dic[emotion] = sentiment_dic[emotion] + 1\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd3d19d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "sentiment_words = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4079d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sentiment_dic = dict(sorted(sentiment_dic.items(),key = operator.itemgetter(1), reverse=True))\n",
    "count = 0  # 记录已经遍历了多少个key值\n",
    "for key in sorted_sentiment_dic:\n",
    "    if count >= 700:  # 如果已经遍历了500个key值，则退出循环\n",
    "        break\n",
    "    if key not in sentiment_words:\n",
    "        sentiment_words.append(key)  # 打印当前key值\n",
    "        count += 1  # 计数器加1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6643a11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiment_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd1bca",
   "metadata": {},
   "source": [
    "将情感词写入txt文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066cc6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment_2019_12.txt','w') as f:\n",
    "    for word in sentiment_words:\n",
    "        f.write(word + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5af7b",
   "metadata": {},
   "source": [
    "写入前判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e0af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment_2019_12.txt','w') as f:\n",
    "    for word in sentiment_words:\n",
    "        # 将文件指针移动到文件的开头\n",
    "        f.seek(0)\n",
    "        # 读取文件中的所有内容\n",
    "        contents = f.read()\n",
    "        # 如果文件不包含指定的字符串\n",
    "        if word not in contents:\n",
    "            # 将指定字符串写入文件\n",
    "            f.write(word + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad89fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_without_stopwords = []\n",
    "\n",
    "for text in text_list:\n",
    "    words = [word for word in jieba.cut(text) if word not in stop_words]  #删除停用词\n",
    "    text_without_stopwords.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc7037d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('no_stopwords.txt','w', encoding = 'utf-8') as f:\n",
    "    for text in text_without_stopwords:\n",
    "        text_s = \"\".join(text)\n",
    "        for entity in entities:\n",
    "            text_s = text_s.replace(entity, \"2019-12\")\n",
    "        f.write(text_s + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dc0ccc",
   "metadata": {},
   "source": [
    "## 获取所有的情感词和实体词之间的平均距离，得到计算共现接近度时的窗口大小\n",
    "此处可以看出窗口的大小应该为4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf999fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最近的情感词汇与 2019-12 的平均距离为： 3.862190812720848\n"
     ]
    }
   ],
   "source": [
    "# 读取 no_stopwords 文件\n",
    "with open(\"no_stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 初始化列表变量\n",
    "distances = []\n",
    "\n",
    "# 遍历每一行文本\n",
    "for line in lines:\n",
    "    # 初始化变量\n",
    "    min_distance = float(\"inf\")\n",
    "    nearest_word = \"\"\n",
    "    \n",
    "    # 遍历每一个词语\n",
    "    words = line.strip().split()\n",
    "    for i in range(len(words)):\n",
    "        if words[i] == \"2019-12\":\n",
    "            # 查找最近的情感词汇\n",
    "            for j in range(max(0, i-5), min(len(words), i+6)):\n",
    "                if words[j] in sentiment_words:\n",
    "                    # 计算情感词汇与实体词之间的距离\n",
    "                    distance = abs(j - i)\n",
    "                    # 更新最小距离和最近情感词汇\n",
    "                    if distance < min_distance:\n",
    "                        min_distance = distance\n",
    "                        nearest_word = words[j]\n",
    "    \n",
    "    # 将最小距离添加到列表中\n",
    "    if nearest_word:\n",
    "        distances.append(min_distance)\n",
    "\n",
    "# 计算平均距离\n",
    "if distances:\n",
    "    avg_distance = sum(distances) / len(distances)\n",
    "    print(\"最近的情感词汇与 2019-12 的平均距离为：\", avg_distance)\n",
    "else:\n",
    "    print(\"未找到匹配的情感词汇。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b7e89b",
   "metadata": {},
   "source": [
    "最近的情感词汇与 2019-12 的平均距离为： 3.862190812720848"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8737b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'非常_聪明': 1, '厉害': 1, '美丽': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551fcae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import thulac\n",
    "\n",
    "thu = thulac.thulac()\n",
    "\n",
    "sentiment_dic = {}\n",
    "\n",
    "for text in text_list:\n",
    "    print(text)\n",
    "    # 对文本进行分词和词性标注\n",
    "    words_with_pos = thu.cut(text)\n",
    "\n",
    "    # 将实体替换为“2019-12”\n",
    "    has_entity = False\n",
    "    original_entity = None\n",
    "    #for text in test_list:\n",
    "    for i in range(len(words_with_pos)):\n",
    "        if words_with_pos[i][0] in entities:\n",
    "            has_entity = True\n",
    "            original_entity = words_with_pos[i][0]\n",
    "            words_with_pos[i] = ('2019-12','entity')\n",
    "            #print(words_with_pos)\n",
    "\n",
    "\n",
    "    # 根据形容词和副词提取情感词\n",
    "    emotions = []\n",
    "    for i in range(len(words_with_pos)):\n",
    "        word, pos = words_with_pos[i]\n",
    "        if pos.startswith('a'):\n",
    "            # 如果当前词是形容词\n",
    "            if i == 0 or not words_with_pos[i-1][1].startswith('d'):\n",
    "                # 如果当前词是第一个词或前面没有副词，则将当前词加入情感词\n",
    "                emotions.append(word)\n",
    "            else:\n",
    "                # 否则，将副词和形容词组合成情感词\n",
    "                emotions.append(words_with_pos[i-1][0] + '_' + word)\n",
    "        elif pos.startswith('d'):\n",
    "            # 如果当前词是副词，不处理\n",
    "            continue\n",
    "\n",
    "    # 将情感词存储进字典中\n",
    "    if has_entity:\n",
    "        entity = original_entity \n",
    "        if entity not in sentiment_dic:\n",
    "            sentiment_dic[entity] = []\n",
    "        sentiment_dic[entity].extend(emotions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1b1d1e",
   "metadata": {},
   "source": [
    "word2vec模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2f7fd9",
   "metadata": {},
   "source": [
    "窗口大小为2的共现矩阵示例代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f6c03",
   "metadata": {},
   "source": [
    "肺炎，口罩，感染"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6aa449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29072\\3401462416.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mco_occurrence\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# 计算单词对之间的KL散度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mkl_divergence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoint_prob\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoint_prob\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mco_occurrence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[1;31m# 将计算结果存储在共现矩阵中\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import math\n",
    "\n",
    "# 读取文件内容\n",
    "with open(\"no_stopwords_2019_12.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 将文本分割成句子列表\n",
    "sentences = text.split(\"\\n\")\n",
    "\n",
    "month = ['2019-12']\n",
    "sentiment_in_window = []\n",
    "\n",
    "# 加载预训练的BERT模型和tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "# 定义实体列表和情感词列表\n",
    "\n",
    "\n",
    "# 初始化共现矩阵\n",
    "co_matrix = np.zeros((len(entities), len(sentiment_words)))\n",
    "\n",
    "# 定义窗口大小\n",
    "window_size = 4\n",
    "\n",
    "# 遍历每个句子\n",
    "for sentence in sentences:\n",
    "    # 将句子拆分成单词列表\n",
    "    words = sentence.split()\n",
    "\n",
    "    # 遍历每个单词\n",
    "    for i, word in enumerate(words):\n",
    "        # 如果单词不在实体列表中，则跳过\n",
    "        if word not in month:\n",
    "            continue\n",
    "\n",
    "        # 计算单词在句子中的上下文窗口范围\n",
    "        start_index = max(0, i - window_size)\n",
    "        end_index = min(len(words) - 1, i + window_size)\n",
    "\n",
    "        # 遍历窗口内的单词\n",
    "        for j in range(start_index, end_index + 1):\n",
    "            # 如果单词不在情感词列表中，则跳过\n",
    "            if words[j] not in sentiment_words:\n",
    "                continue\n",
    "\n",
    "            # 将两个单词拼接成一个字符串，用[SEP]隔开\n",
    "            text = word + \"[SEP]\" + words[j]\n",
    "\n",
    "            # 使用tokenizer将文本转换为BERT输入格式\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "            # 使用BERT模型计算输入文本的表示向量\n",
    "            outputs = model(**inputs)[0]\n",
    "            embeddings = outputs.mean(dim=1).squeeze()\n",
    "\n",
    "            # 计算单词对之间的共现相似度\n",
    "            joint_prob = torch.cosine_similarity(embeddings[0], embeddings[1], dim=0).item()\n",
    "\n",
    "            # 计算单词对之间的co-occurrence\n",
    "            co_occurrence = 1 / (abs(i - j) + 1)\n",
    "            if(co_occurrence != 0):\n",
    "            # 计算单词对之间的KL散度\n",
    "                kl_divergence = joint_prob * math.log(joint_prob / co_occurrence)\n",
    "\n",
    "                # 将计算结果存储在共现矩阵中\n",
    "                row_index = month.index(word)\n",
    "                col_index = sentiment_words.index(words[j])\n",
    "                co_matrix[row_index, col_index] += kl_divergence\n",
    "\n",
    "# 打印共现矩阵\n",
    "print(co_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca025861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\临川Ray\\AppData\\Local\\Temp\\ipykernel_18100\\3680419168.py:40: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  ft_model = FastText.load_fasttext_format(model_path)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/pretrained/fasttext/model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18100\\3680419168.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# 加载FastText词向量模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"path/to/pretrained/fasttext/model\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mft_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFastText\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_fasttext_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m# 将实体和情感词转换为词向量\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1519\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m                 )\n\u001b[1;32m-> 1521\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1523\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\lib\\site-packages\\gensim\\models\\fasttext.py\u001b[0m in \u001b[0;36mload_fasttext_format\u001b[1;34m(cls, model_file, encoding)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m         \"\"\"\n\u001b[1;32m--> 580\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_facebook_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     @utils.deprecated(\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\lib\\site-packages\\gensim\\models\\fasttext.py\u001b[0m in \u001b[0;36mload_facebook_model\u001b[1;34m(path, encoding)\u001b[0m\n\u001b[0;32m    726\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m     \"\"\"\n\u001b[1;32m--> 728\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_load_fasttext_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\lib\\site-packages\\gensim\\models\\fasttext.py\u001b[0m in \u001b[0;36m_load_fasttext_format\u001b[1;34m(model_file, encoding, full_model)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m     \"\"\"\n\u001b[1;32m--> 807\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fasttext_bin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfull_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m     fobj = _shortcut_open(\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/pretrained/fasttext/model'"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "from gensim.models import FastText\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# 读取文本文件中的所有文本\n",
    "with open(\"no_stopwords_2019_12.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 分词并标注词性\n",
    "words = pseg.cut(text)\n",
    "\n",
    "# 找到所有与实体相邻的形容词和副词\n",
    "window_size = 4\n",
    "entity = \"2019-12\"\n",
    "adj_adv_list = []\n",
    "for i, (word, pos) in enumerate(words):\n",
    "    if word == entity:\n",
    "        left = max(0, i - window_size)\n",
    "        right = min(i + window_size, len(words))\n",
    "        for j in range(left, right):\n",
    "            if j != i and 0 <= j < len(words):\n",
    "                if words[j].flag in [\"a\", \"ad\"]:\n",
    "                    adj_adv_list.append((words[j].word, words[j].flag))\n",
    "print(\"finish finding adjs and advs\")\n",
    "\n",
    "# 将形容词和副词组合成“副词_形容词”的形式\n",
    "new_words = []\n",
    "for i in range(len(adj_adv_list)):\n",
    "    if i == 0 or adj_adv_list[i-1][1] == \"d\":\n",
    "        new_words.append(adj_adv_list[i][0])\n",
    "    else:\n",
    "        new_words[-1] += \"_\" + adj_adv_list[i][0]\n",
    "\n",
    "# 找到所有出现在情感词列表中的新词语\n",
    "sentiment_in_window = list(set(new_words).intersection(set(sentiment_words)))\n",
    "print(\"finish finding sentiment in window\")\n",
    "\n",
    "# 加载FastText词向量模型\n",
    "model_path = \"path/to/pretrained/fasttext/model\"\n",
    "ft_model = FastText.load_fasttext_format(model_path)\n",
    "\n",
    "# 将实体和情感词转换为词向量\n",
    "entity_vector = ft_model.wv.get_vector(entity)\n",
    "sentiment_vectors = {}\n",
    "for word in sentiment_in_window:\n",
    "    sentiment_vectors[word] = ft_model.wv.get_vector(word)\n",
    "\n",
    "# 计算每个情感词与实体的联合接近度和KL散度\n",
    "joint_proximity = {}\n",
    "kl_divergence = {}\n",
    "for word in sentiment_in_window:\n",
    "    vec1 = sentiment_vectors[word]\n",
    "    vec2 = entity_vector\n",
    "    joint_proximity[word] = 1 / (1 + np.exp(-np.dot(vec1, vec2)))\n",
    "    kl_divergence[word] = -np.dot(joint_proximity[word], np.log(joint_proximity[word]))\n",
    "\n",
    "# 输出结果\n",
    "print(kl_divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e77254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "from gensim.models import FastText\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# 读取文本文件中的所有文本\n",
    "with open(\"no_stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 分词并标注词性\n",
    "words = pseg.cut(text)\n",
    "\n",
    "# 找到所有与实体相邻的形容词和副词\n",
    "window_size = 4\n",
    "entity = [\"2019-12\", \"2020-01\", \"2020-05\", \"2020-11\"]\n",
    "adj_adv_list = []\n",
    "for i, (word, pos) in enumerate(words):\n",
    "    if word is in entity:\n",
    "        left = max(0, i - window_size)\n",
    "        right = min(i + window_size, len(words))\n",
    "        for j in range(left, right):\n",
    "            if j != i and 0 <= j < len(words):\n",
    "                if words[j].flag in [\"a\", \"ad\"]:\n",
    "                    adj_adv_list.append((words[j].word, words[j].flag))\n",
    "print(\"finish finding adjs and advs\")\n",
    "\n",
    "# 将形容词和副词组合成“副词_形容词”的形式\n",
    "new_words = []\n",
    "for i in range(len(adj_adv_list)):\n",
    "    if i == 0 or adj_adv_list[i-1][1] == \"d\":\n",
    "        new_words.append(adj_adv_list[i][0])\n",
    "    else:\n",
    "        new_words[-1] += \"_\" + adj_adv_list[i][0]\n",
    "\n",
    "# 找到所有出现在情感词列表中的词语\n",
    "sentiment_in_window = list(set(new_words).intersection(set(sentiment_words)))\n",
    "print(\"finish finding sentiment in window\")\n",
    "\n",
    "# 加载FastText词向量模型\n",
    "model_path = \"path/to/pretrained/fasttext/model\"\n",
    "ft_model = FastText.load_fasttext_format(model_path)\n",
    "\n",
    "# 将实体和情感词转换为词向量\n",
    "entity_vector = ft_model.wv.get_vector(entity)\n",
    "sentiment_vectors = {}\n",
    "for word in sentiment_in_window:\n",
    "    sentiment_vectors[word] = ft_model.wv.get_vector(word)\n",
    "\n",
    "# 计算每个情感词与实体的联合接近度和KL散度\n",
    "joint_proximity = {}\n",
    "kl_divergence = {}\n",
    "for word in sentiment_in_window:\n",
    "    vec1 = sentiment_vectors[word]\n",
    "    vec2 = entity_vector\n",
    "    joint_proximity[word] = 1 / (1 + np.exp(-np.dot(vec1, vec2)))\n",
    "    kl_divergence[word] = -np.dot(joint_proximity[word], np.log(joint_proximity[word]))\n",
    "\n",
    "# 输出结果\n",
    "print(kl_divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ad2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_in_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfd816b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95504577",
   "metadata": {},
   "source": [
    "GloVe算法示例代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2675c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "# Assuming your co-occurrence matrix is stored in the variable `co_occurrence_matrix`\n",
    "\n",
    "# First, convert the co-occurrence matrix to a sparse matrix format\n",
    "co_occurrence_matrix_sparse = scipy.sparse.csr_matrix(co_occurrence_matrix)\n",
    "\n",
    "# Next, create a corpus object from the co-occurrence matrix\n",
    "corpus = Corpus()\n",
    "corpus.fit(co_occurrence_matrix_sparse, window=10)\n",
    "\n",
    "# Finally, train the GloVe model on the corpus to get word vectors\n",
    "glove_model = Glove(no_components=50, learning_rate=0.05)\n",
    "glove_model.fit(corpus.matrix, epochs=100, no_threads=4, verbose=True)\n",
    "\n",
    "# Now you can access the word vectors for each word\n",
    "word_vectors = glove_model.word_vectors\n",
    "\n",
    "# Assuming you have a list of emotion words called `emotion_words`\n",
    "for word in emotion_words:\n",
    "    word_vector = word_vectors[glove_model.dictionary[word]]\n",
    "    print(f\"The vector for '{word}' is: {word_vector}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a525ae34",
   "metadata": {},
   "source": [
    "得到月份向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b3ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "# Assuming your co-occurrence matrix is stored in the variable `co_occurrence_matrix`\n",
    "\n",
    "# First, create a corpus object from the rows of the co-occurrence matrix corresponding to the month words\n",
    "month_corpus = Corpus()\n",
    "month_corpus.fit(co_occurrence_matrix[:, :12], window=10)\n",
    "\n",
    "# Finally, train the GloVe model on the month corpus to get month vectors\n",
    "month_glove_model = Glove(no_components=50, learning_rate=0.05)\n",
    "month_glove_model.fit(month_corpus.matrix, epochs=100, no_threads=4, verbose=True)\n",
    "\n",
    "# Now you can access the month vectors for each month\n",
    "month_vectors = month_glove_model.word_vectors\n",
    "\n",
    "# Assuming you have a list of month words called `month_words`\n",
    "for word in month_words:\n",
    "    word_vector = month_vectors[month_glove_model.dictionary[word]]\n",
    "    print(f\"The vector for '{word}' is: {word_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d6c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "\n",
    "def glove(cooccur, vec_size=50, alpha=0.75, x_max=100, learning_rate=0.05, max_iter=100):\n",
    "    # 统计每个单词的出现次数（根据共现矩阵得到）\n",
    "    word_freq = np.sum(cooccur, axis=1)\n",
    "    # 初始化权重矩阵 W 和偏置 b\n",
    "    W = np.random.normal(size=(len(word_freq), vec_size))\n",
    "    b = np.random.normal(size=len(word_freq))\n",
    "    # 初始化梯度平方和\n",
    "    gradsq_W = np.ones((len(word_freq), vec_size))\n",
    "    gradsq_b = np.ones(len(word_freq))\n",
    "    # 定义 cost 函数\n",
    "    def cost(f, x):\n",
    "        diff = (np.dot(W[f], W[x]) + b[f] + b[x] - np.log(cooccur[f, x]))\n",
    "        weight = (cooccur[f, x] / x_max) ** alpha if cooccur[f, x] < x_max else 1\n",
    "        return 0.5 * weight * diff ** 2\n",
    "    # 训练模型\n",
    "    for i in range(max_iter):\n",
    "        cost_total = 0\n",
    "        # 遍历数据\n",
    "        for f in range(len(word_freq)):\n",
    "            for x in range(len(word_freq)):\n",
    "                if cooccur[f, x] > 0:\n",
    "                    # 计算 cost 和权重\n",
    "                    c = cost(f, x)\n",
    "                    cost_total += c\n",
    "                    weight = (cooccur[f, x] / x_max) ** alpha if cooccur[f, x] < x_max else 1\n",
    "                    # 计算梯度\n",
    "                    grad_f = weight * c * W[x]\n",
    "                    grad_x = weight * c * W[f]\n",
    "                    grad_b_f = weight * c\n",
    "                    grad_b_x = weight * c\n",
    "                    # 更新参数和梯度平方和\n",
    "                    W[f] -= (learning_rate * grad_f / np.sqrt(gradsq_W[f]))\n",
    "                    W[x] -= (learning_rate * grad_x / np.sqrt(gradsq_W[x]))\n",
    "                    b[f] -= (learning_rate * grad_b_f / np.sqrt(gradsq_b[f]))\n",
    "                    b[x] -= (learning_rate * grad_b_x / np.sqrt(gradsq_b[x]))\n",
    "                    gradsq_W[f] += np.square(grad_f)\n",
    "                    gradsq_W[x] += np.square(grad_x)\n",
    "                    gradsq_b[f] += grad_b_f ** 2\n",
    "                    gradsq_b[x] += grad_b_x ** 2\n",
    "        print('Iter: {} Cost: {}'.format(i, cost_total))\n",
    "    # 对 W 和 b 做 SVD 分解\n",
    "    U, S, V = svd(W)\n",
    "    # 返回词向量矩阵和偏置向量\n",
    "    return U * np.sqrt(S)[:, np.newaxis], b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ffe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 定义共现矩阵\n",
    "cooc_mat = np.array([\n",
    "    [0.0, 0.2, 0.3, 0.1],\n",
    "    [0.2, 0.0, 0.4, 0.5],\n",
    "    [0.3, 0.4, 0.0, 0.2]\n",
    "])\n",
    "\n",
    "# 定义每个词的名称\n",
    "emotion_words = [\"高兴\", \"愤怒\", \"悲伤\"]\n",
    "months = [\"2019-12\", \"2020-05\", \"2020-11\"]\n",
    "\n",
    "# 训练词向量\n",
    "model = KeyedVectors(size=100, window=5, min_count=1)\n",
    "model.build_vocab_from_freq({w: 1 for w in emotion_words + months})\n",
    "model.train(\n",
    "    sentences=[emotion_words + months],\n",
    "    total_examples=1,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# 获取情感词的向量\n",
    "for word in emotion_words:\n",
    "    print(\"情感词：{}，向量：{}\".format(word, model.wv[word]))\n",
    "\n",
    "# 获取月份的向量\n",
    "for month in months:\n",
    "    print(\"月份：{}，向量：{}\".format(month, model.wv[month]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4046ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# 构建共现矩阵\n",
    "cooc_mat = np.array([\n",
    "    [0.0, 0.2, 0.3, 0.1],\n",
    "    [0.2, 0.0, 0.4, 0.5],\n",
    "    [0.3, 0.4, 0.0, 0.2]\n",
    "])\n",
    "\n",
    "# 定义模型参数\n",
    "vector_size = 100\n",
    "alpha = 0.75\n",
    "x_max = 100\n",
    "epochs = 100\n",
    "\n",
    "# 初始化词向量和偏置项\n",
    "w = np.random.uniform(-0.5 / vector_size, 0.5 / vector_size, (cooc_mat.shape[0], vector_size))\n",
    "b = np.random.uniform(-0.5 / vector_size, 0.5 / vector_size, (cooc_mat.shape[0],))\n",
    "\n",
    "# 定义权重函数\n",
    "def weight_func(x):\n",
    "    return np.power(np.clip(x / x_max, 0.0, 1.0), alpha)\n",
    "\n",
    "# 计算权重矩阵\n",
    "weight_mat = weight_func(cooc_mat)\n",
    "\n",
    "# 将权重矩阵转换为稀疏矩阵\n",
    "weight_mat = sparse.csr_matrix(weight_mat)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(epochs):\n",
    "    # 计算词向量之间的内积和偏置项之和\n",
    "    x = np.dot(w, w.T) + b.reshape((-1, 1)) + b.reshape((1, -1))\n",
    "    # 将内积和偏置项之和转换为稀疏矩阵\n",
    "    x = sparse.csr_matrix(x)\n",
    "    # 计算误差\n",
    "    err = weight_mat.multiply(x - np.log(cooc_mat))\n",
    "    # 更新词向量和偏置项\n",
    "    grad_w = err.dot(w)\n",
    "    grad_b = err.sum(axis=1)\n",
    "    w -= 0.01 * grad_w\n",
    "    b -= 0.01 * grad_b\n",
    "\n",
    "# 获取情感词的向量\n",
    "for word_idx, word in enumerate(emotion_words):\n",
    "    print(\"情感词：{}，向量：{}\".format(word, w[word_idx]))\n",
    "\n",
    "# 获取月份的向量\n",
    "for month_idx, month in enumerate(months):\n",
    "    print(\"月份：{}，向量：{}\".format(month, w[len(emotion_words) + month_idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c40486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "\n",
    "\n",
    "def glove(cooccur, vec_size=50, alpha=0.75, x_max=100, learning_rate=0.05, max_iter=100):\n",
    "    # 统计每个单词的出现次数（根据共现矩阵得到）\n",
    "    word_freq = np.sum(cooccur, axis=1)\n",
    "    # 初始化权重矩阵 W 和偏置 b\n",
    "    W = np.random.normal(size=(len(word_freq), vec_size))\n",
    "    b = np.random.normal(size=len(word_freq))\n",
    "    # 初始化梯度平方和\n",
    "    gradsq_W = np.ones((len(word_freq), vec_size))\n",
    "    gradsq_b = np.ones(len(word_freq))\n",
    "    # 定义 cost 函数\n",
    "    def cost(f, x):\n",
    "        diff = (np.dot(W[f], W[x]) + b[f] + b[x] - np.log(cooccur[f, x]))\n",
    "        weight = (cooccur[f, x] / x_max) ** alpha if cooccur[f, x] < x_max else 1\n",
    "        return 0.5 * weight * diff ** 2\n",
    "    # 训练模型\n",
    "    for i in range(max_iter):\n",
    "        cost_total = 0\n",
    "        # 随机遍历数据\n",
    "        for f in range(len(word_freq)):\n",
    "            for x in range(len(word_freq)):\n",
    "                if cooccur[f, x] > 0:\n",
    "                    # 计算 cost 和权重\n",
    "                    c = cost(f, x)\n",
    "                    cost_total += c\n",
    "                    weight = (cooccur[f, x] / x_max) ** alpha if cooccur[f, x] < x_max else 1\n",
    "                    # 计算梯度\n",
    "                    grad_f = weight * c * W[x]\n",
    "                    grad_x = weight * c * W[f]\n",
    "                    grad_b_f = weight * c\n",
    "                    grad_b_x = weight * c\n",
    "                    # 更新参数和梯度平方和\n",
    "                    W[f] -= (learning_rate * grad_f / np.sqrt(gradsq_W[f]))\n",
    "                    W[x] -= (learning_rate * grad_x / np.sqrt(gradsq_W[x]))\n",
    "                    b[f] -= (learning_rate * grad_b_f / np.sqrt(gradsq_b[f]))\n",
    "                    b[x] -= (learning_rate * grad_b_x / np.sqrt(gradsq_b[x]))\n",
    "                    gradsq_W[f] += np.square(grad_f)\n",
    "                    gradsq_W[x] += np.square(grad_x)\n",
    "                    gradsq_b[f] += grad_b_f ** 2\n",
    "                    gradsq_b[x] += grad_b_x ** 2\n",
    "        print('Iter: {} Cost: {}'.format(i, cost_total))\n",
    "    # 对 W 和 b 做 SVD 分解\n",
    "    U, S, V = svd(W)\n",
    "    # 返回词向量矩阵和偏置向量\n",
    "    return U * np.sqrt(S)[:, np.newaxis], b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebefea7",
   "metadata": {},
   "source": [
    "计算cosine相似度的示例代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b960a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assuming you have two dictionaries containing the word vectors for the months and emotion words, called `month_vectors` and `emotion_vectors`, respectively\n",
    "\n",
    "# Create a matrix to store the cosine similarities\n",
    "cosine_similarities_matrix = np.zeros((len(emotion_vectors), len(month_vectors)))\n",
    "\n",
    "# Calculate the cosine similarity between each emotion word and each month\n",
    "for i, (emotion_word, emotion_vector) in enumerate(emotion_vectors.items()):\n",
    "    for j, (month_word, month_vector) in enumerate(month_vectors.items()):\n",
    "        cosine_similarity_value = cosine_similarity(emotion_vector.reshape(1,-1), month_vector.reshape(1,-1))[0][0]\n",
    "        cosine_similarities_matrix[i][j] = cosine_similarity_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9160af",
   "metadata": {},
   "source": [
    "生成字典示例代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a906499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a cosine similarities matrix called `cosine_similarities_matrix`, and lists of the month words and emotion words called `month_words` and `emotion_words`, respectively\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "results_dict = {}\n",
    "\n",
    "# Find the emotion word with the highest similarity for each month\n",
    "for i, month_word in enumerate(month_words):\n",
    "    max_similarity = -1\n",
    "    max_emotion_word = \"\"\n",
    "    for j, emotion_word in enumerate(emotion_words):\n",
    "        similarity = cosine_similarities_matrix[j][i]\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            max_emotion_word = emotion_word\n",
    "    \n",
    "    # Add the result to the dictionary\n",
    "    if month_word not in results_dict:\n",
    "        results_dict[month_word] = {}\n",
    "    results_dict[month_word][max_emotion_word] = max_similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
